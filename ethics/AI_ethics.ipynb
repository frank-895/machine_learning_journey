{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIKjM2Lm9QWHi4Izs+pqH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-895/machine_learning_journey/blob/main/ethics/AI_ethics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Ethics"
      ],
      "metadata": {
        "id": "0ba53KCyOmot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are my notes about data science ethics, an area I consider important especially when studying groundbreaking technologies like machine learning.\n",
        "\n",
        "These notes are from the FastAI bonus chapter on ethics, specifically this YouTube Video: [Ethics for Data Science](https://www.youtube.com/watch?v=krIVOb23EH8).\n",
        "\n",
        "This notebook focuses on current ethical dilemmas surrounding data science - not future ethical problems. While technology has a done a lot of good for the world, it has and continues to cause significant harm."
      ],
      "metadata": {
        "id": "hpJcnJ4DS9qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Ethics?"
      ],
      "metadata": {
        "id": "h-_vIOv-SZzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ethics is the discipline dealing with what is good and bad; a set of moral principles.\n",
        "\n",
        "Ethics is not fixed, like religion or law.\n",
        "1. It is a set of well-founded standards of right and wrong, prescribing what humans ought to do.\n",
        "2. It is the study and development of one's ethical standards.\n",
        "\n",
        "Let's have a look at two ethical philosophies and how they can be applied to our projects:\n",
        "\n",
        "1. **Consequentialism/utiliatrianism** - maximising good.\n",
        "  - Who will be directly and indirectly affected by the project?\n",
        "  - Will the effects in aggregate create more good than harm?\n",
        "  - Are we thinking about all types of harm/benefit?\n",
        "    - psychological\n",
        "    - political\n",
        "    - environmental\n",
        "    - moral\n",
        "    - cognitive\n",
        "    - emotional\n",
        "    - institutional\n",
        "    - cultural\n",
        "  - Do the risks of harm/benefit fall disproportionately on the least/most powerful in society?\n",
        "  - Have we considered **dual-use** - i.e., could the project also be used for harm?\n",
        "\n",
        "2. **Deontologicalism** - adhering to the 'right'.\n",
        "  - What rights of others and duties to others must we respect?\n",
        "  - How might the dignity and autonomy of each stakeholder be impacted by this project?\n",
        "  - What considerations of trust & justice are relevant to this project?\n",
        "  - Does this project involve any conflicting stakeholder rights? How can they be prioritised?\n",
        "\n",
        "We can also look at ethics in AI through 5 **ethical lenses**. It is not necessary to choose an ethical philosophy and live by it, but to instead consider our project in as many ways as possible.\n",
        "1. **The rights approach**: Which option best respects the rights of those who have a stake?\n",
        "2. **The justice approach**: Which option treats people equally?\n",
        "3. **The utilitarian approach**: Which option will produce the most good and do the least harm?\n",
        "4. **The common good approach**: Which option best serves the community as a whole, not just some members?\n",
        "5. **The virtue approach**: With option leads me to act as the sort of person I want to be?\n",
        "\n",
        "Lenses 1 and 2 are deontological, while 3 and 4 and consequentialist.\n",
        "\n",
        "It's also good to note that this discussion so far has been a very *western* view of ethics and morality. Their are other worldviews to consider, and when implementing a project, it is important to consider the cultural and ethical lenses of the people who have a stake."
      ],
      "metadata": {
        "id": "_R_n_HtcSHOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why is ethics in data science important?"
      ],
      "metadata": {
        "id": "r3i0jb4fQj51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collection has played a pivotal role in genocides, including the holocaust.\n",
        "\n",
        "IBM used data science to decide whether people were Jewish and whether they should be executed. They produced computer systems that were used in concentration camps and gas chambers - these machines required constant maintenance and an ongoing relationship between user and vendor.\n",
        "\n",
        "This is an important reminder how technology can be used for harm and why it is critical that ethics are considered when using technology."
      ],
      "metadata": {
        "id": "SR4emaydQnNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is speed/hypergrowth related to data ethics?"
      ],
      "metadata": {
        "id": "jLzv5AaTUD3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Super-fast growth requries automation & reliance on metrics.\n",
        "- Prioritising speed above all else doesn't leave time to reflect on ethics.\n",
        "- Problems happen or surface on a large scale if the company grows too quickly."
      ],
      "metadata": {
        "id": "W_GiO64RUHi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "UXz1gx8gNKO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Reliance on metrics is a fundamental challenge for AI.\n",
        "\n",
        "Choosing appropriate metrics is very important when building an AI model, as deep learning is very very effective at optimising metrics. While this is the strength of deep learning, it is also a fundamental challenge, as inappropriate metrics can have a devastating impact.\n",
        "\n",
        "**Overemphasising metrics** can lead to:\n",
        "- manipulation\n",
        "- gaming\n",
        "- focus on short-term goals\n",
        "- unexpected negative consequences\n",
        "\n",
        "Let's have a look at an example, from the 2000s when the UK started focusing intensely on numbers to improve performance in the healthcare system. This project was called \"*What's measured is what matters*\", [link to study here](https://www.researchgate.net/publication/229615633_What's_Measured_Is_What_Matters_Targets_and_Gaming_in_the_English_Public_Health_Care_System).\n",
        "\n",
        "One of the metrics was around emergency department (ED) wait times. By **overemphasising** this metric, the following issues occured:\n",
        "1. Scheduled operations were cancelled to draft extra staff to ED.\n",
        "2. Patients were required to wait in queues of ambulances.\n",
        "3. Stretchers were turned into \"beds\" by putting them in hallways.\n",
        "4. There were big discrepancies in numbers reported by hospitals vs by patients.\n",
        "\n",
        "This is an example of **gaming** occuring due to metrics. The healthcare system did not actually improve, if anything it got worse, as the people in charge were manipulating processes to optimise a single metric.\n",
        "\n",
        "An essay grading software had similar issues in America. Metrics for grading an essay included sentence length, vocabulary, spelling, subject-verb agreement - because these are metrics that are easy to measure. Therefore, it was not possible for the software to measure hard-to-quantify qualitities, like creativity.\n",
        "\n",
        "As such, gibberish essays with sophisticated words scored best - an example of poorly chosen metrics not representative of what a **good** submission **actually** looks like.\n",
        "\n",
        "Goodhart's Law is an important reminder why not to over-rely on metrics:\n",
        "\n",
        "> \"When a measure becomes a target, it ceases to be a good measure.\""
      ],
      "metadata": {
        "id": "4B7YqoVqNPUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A metric is just a proxy for what you care about - and it turns out, its not so easy to measure **what you care about**."
      ],
      "metadata": {
        "id": "NG1ofw32Q10X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedback Loops"
      ],
      "metadata": {
        "id": "ObUFN9hKRkKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our online environments are susceptible to feedback loops, \"when your model is controlling the next round of data you get. The data that is returned quickly becomes flawed by the software itself.\" It's often referred to as **echo chambers**, particularly in social media.\n",
        "\n",
        "For example, recommendation systems use watch time as a **proxy** for how interested we are in something. This leads to conspiracy content performing well, as it encourages it viewers to keep \"uncovering\" more \"information\". This was not an intended consequence when recommendation algorithms were originally built, but an unintended consequence now widely exploited.\n",
        "\n",
        "Our online environments are designed to be addictive and content creators are always trying to **game** the metrics to improve their performance. This makes choosing appropriate metrics even harder.\n",
        "\n",
        "Feedback loops are common with recommendation systems, as they return what the user likes... but also what they are exposed to. It can reinforce and recommend damaging videos/articles/images etc. We can see this through [Meta's role in the Rohingya genocide](https://www.amnesty.org/en/latest/news/2022/09/myanmar-facebooks-systems-promoted-violence-against-rohingya-meta-owes-reparations-new-report/).\n",
        "\n",
        "A good quote from James Grimmelman:\n",
        "> \"These platforms are structurally at war with themselves. The same characteristics that make outrageous & offensive content unaccpetable are what make it go viral in the first place.\"\n",
        "\n",
        "In this way, disinformation is built into modern tech companies and into their business models.\n",
        "\n"
      ],
      "metadata": {
        "id": "hgAOY47YRl3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias"
      ],
      "metadata": {
        "id": "KuILIjSlVCN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gender Bias"
      ],
      "metadata": {
        "id": "hiYKy3NvRy_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commercial computer vision products perform [significantly better on men and on white people](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212) and perform very poorly on women of colour. This research was conducted on several large commerical products and they all showed this significant bias.\n",
        "\n",
        "**What is the source of this problem?**\n",
        "- Generally, unrepresentative datasets which were primarily built on white men. When the benchmark contains bias, this will be perpetuated on a larger scale with machine learning, as the algorithm will optimise to this biased dataset.\n",
        "- Blackbox algorithms can be trained on many variables and cannot be analysed to check for bias.\n",
        "- Generally, bias in technology is sourced from bias in real-life - but, it has the potential to amplify it, especially if algorithms are trained to optimise biased metrics or benchmarks."
      ],
      "metadata": {
        "id": "20yTJDmxR3Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Historical Bias"
      ],
      "metadata": {
        "id": "7m_jztXXScnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historical bias is:\n",
        "> \"a fundamental, structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection.\"\n",
        "\n",
        "An example of this is with the [COMPAS recidivism algorithm](https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/) used in the US to predict whether someone will re-offend to decide if they should pay bail. This algorithm was found to not only be supremely racist but also to be no more effective than guessing. It was upheld even after extensive research demonstrating its flaws."
      ],
      "metadata": {
        "id": "gUtbICawSb5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measurement Bias"
      ],
      "metadata": {
        "id": "0Hy935teR3-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measurement bias is:\n",
        "> when data collection methods systematically distort the true values of what is being measured.\n",
        "\n",
        "An example of this is in this paper: [Does Machine Learning Automate Moral Hazard and Error?](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) This paper discusses an algorithm suggested to predict a person's risk of stroke to improve efficiency in the ED. What they found was that a number of irrelevant factors where most predictive of stroke, like \"accidental injury\" and \"colonoscopy\".\n",
        "\n",
        "*Why is this?* The researchers hadn't measured the chance of stroke, but the chance someone had symptoms, went to the doctor, got tests and recieved a diagnosis. And this is influenced by **MANY** other factors then just the chance of stroke, including: race, class, gender, and health insurance."
      ],
      "metadata": {
        "id": "_G7LDTHAR8Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Racial Bias"
      ],
      "metadata": {
        "id": "jCDwNHfLR-Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Humans are very biased, see [these researched and peer reviewed examples](https://www.nytimes.com/2015/01/04/upshot/the-measuring-sticks-of-racial-bias-.html) of racial bias:\n",
        "\n",
        ">\"When doctors were shown patient histories and asked to make judgments about heart disease, they were much less likely to recommend cardiac catheterization (a helpful procedure) to black patients\"\n",
        "\n",
        "> \"When whites and blacks were sent to bargain for a used car, blacks were offered initial prices roughly $700 higher, and they received far smaller concessions.\""
      ],
      "metadata": {
        "id": "MRk45vDMSAcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If humans are biased, why does algorithmic bias matter?"
      ],
      "metadata": {
        "id": "ABtykBsXSCKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Machine learning can amplify bias** - [Bias in bios](https://arxiv.org/abs/1901.09451) showed that the gender imbalance in medicine was amplified and made even worse when asking an algorithm to predict a person's job title.\n",
        "2. **Algorithms are used differently than human decision makers** - people are more likely to assume algorithms are objective, algorithms are more likely to implemented with no appeals process, algorithms are often used at scale, and algorithmic systems are cheap.\n",
        "3. **Machine learning can create feedback loops**.\n",
        "4. **Technology is power. And that comes with responsibility**."
      ],
      "metadata": {
        "id": "lXiNnU2QVEQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disinformation"
      ],
      "metadata": {
        "id": "8ArpWJwKzY0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disinformation is:\n",
        "> false or misleading information that is deliberately created and spread to decieve people.\n",
        "\n",
        "Disinformation can include so-called \"fake news\", where a single article or blog post is labelled as incorrect. However, on a larger scale, disinformation includes orchestrated campaigns of manipulation.\n",
        "\n",
        "AI can be used to generate compelling but false information that can be dispelled at a large scale. It can be subtle, even involving 2 language models arguing with each other, where one slightly takes the upper hand. False profiles for people can be generated, who appear to be reliable or professionals.\n",
        "\n",
        "In 2017, it is estimated that nearly 18 million out of 22 million comments deciding  net-neutrality laws in the US were fabricated, mostly by internet providers, [source](https://www.wired.com/story/isps-funded-85-million-fake-comments-opposing-net-neutrality/). This was prior to the advent of generative AI, demonstrating the real impact of computer-generated content on our laws, which is expected to worsen unless legislative action is taken.\n",
        "\n",
        "As humans, we've evolved to generate our opinions based on our in-group and to disagree with those in out-group. AI has the potential to amplify these differences and spread false and misleading information on a very large scale.\n",
        "\n",
        "AI allows us to make forgery convincing, inexpensive, and automated. Solutions such as digital signatures have been suggested to address these concerns. It has also been suggested that **disinformation needs to be treated a cybersecurity problem.**"
      ],
      "metadata": {
        "id": "N4h3UW9k0J92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diversity"
      ],
      "metadata": {
        "id": "eux6wkhAEVL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning research is not very diverse ([source](https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996)), particularly for women and people of colour.\n",
        "\n",
        "An important statistic from this article (from 2022) is that 41% of women working in tech end up leaving, compared to 17% of men. Increasing the number of women learning coding and going into tech is not going to fix this problem.\n",
        "\n",
        "It's important to have diversity on teams. The first female engineer at Quora implemented a 'block' feature - something that otherwise would not have been implemented by her male colleagues. This is an important reminder why diverse experiences are valuable to a project, to a company, and to society."
      ],
      "metadata": {
        "id": "GJIFML2qEYKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What can we do as engineers?"
      ],
      "metadata": {
        "id": "g5FETnyBVneO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vet the company you're joining for their ethics. We normally have lots of options and we can use our skill as leverage.\n",
        "- While pressure from management might give us some leeway for unethical behaviour, it is important to be personally accountable for our actions and the harms we can cause.\n",
        "- Talk to experts **and** people directly impacted by technology. Get feedback before and after release.\n",
        "- Ask yourself questions:\n",
        "  - *Should we even be doing this?* Not everything that **can** be done **should** be done.\n",
        "  - *What bias is in the data?* There will **always** be some level of bias in the data but it is important to understand how it is collected, etc.\n",
        "  - *Can the code and data be audited?* Propietary black boxes can be very damaging and difficult to trace, monitor, and evaluate.\n",
        "  - *What are error rates for different sub-groups?* Are certain groups of data underperforming?\n",
        "  - *What is the accruacy of a simple rule-based alternative?* Have a baseline - if your complex model does not outperform the baseline, why are we even doing this?\n",
        "  - *What processes are in place to handle appeals or mistakes?* There will be problems and it is important to have a robust system for them to be identified and rectified.\n",
        "- Implement some tools:\n",
        "  - **Ethical risk sweeping**: Implement regular ethical risk-sweeping - just as you would perform cybersecurity penetration testing (regardless of whether you find something), we should do the same with ethical risks. Assume you missed risks in initial development and reward people for spotting them.\n",
        "  - **Expanding the ethical circle**: Whose interests, skills, experiences, and values have we assumed, rather than consulted? Who will be indirectly affected in significant ways? Who might use this product for an unexpected purpose?\n",
        "  - **Think about the terrible people**: Who will want to abuse, steal, misinterpert, hack, destroy, or weaponise what we've built? What rewards/openings has our design inadvertently created?\n",
        "  - **Closing the loop**: Remember that ethical design is never finished. Identify feedback channels for ethical impact. Develop formal procedures and chains of responibility for ethical iteration."
      ],
      "metadata": {
        "id": "PEikEB1CVpgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "2Jraqnt3Tscn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While learning about machine learning and AI, I've been focusing on its applications for good - and there's no doubt that AI has the potential to drastically improve our world and to make society more equal.\n",
        "\n",
        "However, its important to look at its implications. Like all new technologies, AI has its drawbacks, flaws, and misuse potential. This short ethics seminar was an important reminder that our powerful tech companies need to be help account, and that the financial incentive should be for good... not a race to the bottom. There needs to be significant legislative reformation to ensure AI moves the world forward.\n",
        "\n",
        "However, while ethics is a never-ending journey, I feel equipped with some knowledge and certainly some awareness of the implications of the technology I'm studying. This will be an area of continual research for me, as I want to contribute my technological skills for good in the world.\n",
        "\n",
        "And, while the motivations and actions of certain tech companies seems dire and bleak, innovation has always been ahead of legislation. Its important as data scientists and engineers to advocate for ethics on a personal, company and legislative level.\n",
        "\n",
        "When cars were first invented, safety features were sparse as there was no financial incentive to install them (even though they existed). With time, advocacy, and legislation, drastic changes have been made to improve car safety standards. And the common line used in the past was that \"the people who drive cars are dangerous, not the cars themselves\". **This is obviously misleading and irrelevant, yet an enduring sentiment in innovation** It's important that we hold ourselves and our companies to account, as we have a great responsibility to ensure technology is used for good."
      ],
      "metadata": {
        "id": "sz01D41bTuke"
      }
    }
  ]
}