{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3ASXU5y8KEgVoLz2aYXZ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-895/train_learner/blob/main/learner_build.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LgeThwlVMJel"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "metadata": {
        "id": "NSdsRVPpOeFJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Linear Learner Model to Classify Handwritten Digits"
      ],
      "metadata": {
        "id": "7rcxtW18bv77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Data\n",
        "\n",
        "MNIST contains images of handwritten digits. We will use these images to manually create a Learner model that can detect the difference between handwritten numbers."
      ],
      "metadata": {
        "id": "fxwPMXv5pw_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.MNIST)\n",
        "Path.BASE_PATH = path\n",
        "(path).ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "kl7QcvrKOe-3",
        "outputId": "f76a15c8-c246-4780-e3c0-d930942cb6c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='15687680' class='' max='15683414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.03% [15687680/15683414 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('training'),Path('testing')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will download the training and validation images for each number into respective arrays, converting them to a scale of 0 to 1."
      ],
      "metadata": {
        "id": "EhM6KFidPzzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tns = []\n",
        "valid_tns = []\n",
        "# extract and convert to scale of 0 to 1 using tensor operations\n",
        "for i in range(10):\n",
        "  train_tns.append(torch.stack([tensor(Image.open(o)).float()/255 for o in (path/'training'/str(i)).ls()]))\n",
        "  valid_tns.append(torch.stack([tensor(Image.open(o)).float()/255 for o in (path/'testing'/str(i)).ls()]))"
      ],
      "metadata": {
        "id": "woqXylExOjZX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to convert our tensors from a list of matrices to a list of vectors, flattening the pixels in each image to a single row of 28*28 pixels. We use `view` which increases the axis to make it as big as needed to fit all the data. Then, we will create labels for each of the images using integer-encoded class labels for multi-class classification."
      ],
      "metadata": {
        "id": "flEpxrc3RInN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract images and flatten\n",
        "train_x = torch.cat(train_tns).view(-1, 28*28)\n",
        "valid_x = torch.cat(valid_tns).view(-1, 28*28)\n",
        "\n",
        "# extract labels for each image\n",
        "train_y = []\n",
        "valid_y = []\n",
        "for i in range(10):\n",
        "  train_y.extend([i] * len(train_tns[i]))\n",
        "  valid_y.extend([i] * len(valid_tns[i]))\n",
        "\n",
        "# make labels rank-2 tensors\n",
        "train_y = tensor(train_y).unsqueeze(1)\n",
        "valid_y = tensor(valid_y).unsqueeze(1)\n",
        "train_x.shape, train_y.shape"
      ],
      "metadata": {
        "id": "iIj0f50KPp3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9237969a-9cbc-4d53-d206-6708e4186213"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 784]), torch.Size([60000, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`.unsqueeze()` adds a new dimension to train_y, making it a rank-2 tensor, suitable for classification tasks. Because of this, it is easy to create a Dataset for PyTorch where each flattened image is associated with its label."
      ],
      "metadata": {
        "id": "p49yEn1VUKvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dset = list(zip(train_x, train_y))\n",
        "valid_dset = list(zip(valid_x, valid_y))"
      ],
      "metadata": {
        "id": "3jEInr6qPuaO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to begin training our Learner model."
      ],
      "metadata": {
        "id": "uXQlbwTzprAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Creating Model"
      ],
      "metadata": {
        "id": "YejHrg64bnoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise Weights\n",
        "\n",
        "We start by initialising our **weights** with random values from the normal distribution. Each weight will be associated with one of the 28x28 pixel values. We give weights a shape in the 2nd axis of 10, representing the probability the image belongs to each of the 10 classes from 0 to 9. We also need to define bias to ensure the model is suitably flexible, otherwise input of 0 will not be trainable. Together, the weights and bias define our **parameters**."
      ],
      "metadata": {
        "id": "NgGskb-xpuhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(size, std=1.0):\n",
        "  \"\"\"Returns init\"\"\"\n",
        "  return (torch.randn(size)*std).requires_grad_()\n",
        "\n",
        "weights = init_params((28*28, 10))\n",
        "bias = init_params(10)\n",
        "\n",
        "params = weights, bias\n",
        "weights.shape, bias.shape"
      ],
      "metadata": {
        "id": "jhj1cvBLT4Gn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9641453f-cf67-42c2-8b0e-29a78a377b10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([784, 10]), torch.Size([10]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a Prediction\n",
        "\n",
        "Now that we have parameters and input, we can create a model that uses the parameters to classify each input.\n",
        "\n",
        "We will use **matrix multiplication** for this task. We want to find the dot product of the weights and the 784 pixel values, then add the bias to this. We could do this in a number of ways, including using Python `for` loops - but this would be very inefficient. PyTorch is optimised to do the same task using matrix multiplication (`@` operator) extremely quickly on the GPU. We add the bias to the model too."
      ],
      "metadata": {
        "id": "0MmufHvxd5pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_x(x):\n",
        "  \"\"\"Uses the models parameters to predict the classification of an image.\"\"\"\n",
        "  return x@weights + bias"
      ],
      "metadata": {
        "id": "voYpxdYGc3fV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Loss\n",
        "\n",
        "Now, we calculate **loss** to determine how effective our parameters are in classifying handwritten digits. Loss is a value that represents how well (or badly) our model is doing.\n",
        "\n",
        "Because we are performing multi-class classification we will use **cross-entropy loss**, which quantifies how well predicted probability distributions align with the true labels, penalizing incorrect predictions based on their confidence levels."
      ],
      "metadata": {
        "id": "dzy_bVXnevmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(preds, y):\n",
        "  \"\"\"Calculates the cross-entropy loss using a tensor of logits from the model\"\"\"\n",
        "  return torch.nn.functional.cross_entropy(preds, y.squeeze())"
      ],
      "metadata": {
        "id": "VaKNdA61eroi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Gradient\n",
        "\n",
        "Using **stochastic gradient descent** (SGD), which is an **optimiser** function, we will update the parameters in the model in each **epoch** to improve its predictive power. An epoch refers to a single complete pass of the entire training dataset through the model. SGD is an optimisation algorithm through which the parameters are updated by finding the gradient (derivative) of each parameter and gradually adjusting it so it moves towards its optimal value. PyTorch automatically employs **backpropagation** to calculate the gradient of the loss with respect to each weight using the chain rule of calculus because the linear_model is defined using `torch.nn`. PyTorch can very efficiently calculate derivatives.\n",
        "\n",
        "We calculate the loss for **mini-batches** to ensure efficiency and suitable loss predictions. We will use a `DataLoader` object from PyTorch which turns a Python collection into an iterable, connecting each input with its respective label. It will also shuffle the data items in each mini-batch every epoch. The **batch size** is the number of data items in a mini-batch."
      ],
      "metadata": {
        "id": "42qc7EklgwXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a function to compute the models predictions, calculate the loss and compute the gradients with respect to the loss\n",
        "def calc_grad(xb, yb, model):\n",
        "  preds = model(xb)\n",
        "  loss = calc_loss(preds, yb)\n",
        "  loss.backward()"
      ],
      "metadata": {
        "id": "IPeN2AoFgqYN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step the Parameters\n",
        "\n",
        "Now we can update each parameter (called \"stepping\") using the gradient calculated in `calc_grad` and the **learning rate** defined by `lr`.  The learning rate is size of step we take when applying SGD to update the parameters.\n",
        "\n",
        "We will also create some code to define a metric for human consumption called accuracy. This will determine how accurately the model is correctly classifying the images and is more appropriate for human evaluation than loss (which is better for the training process)."
      ],
      "metadata": {
        "id": "Zm2ZAWttkTz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise parameters and 2 DataLoader objects for training set and validaiton set\n",
        "dl = DataLoader(dset, batch_size=256)\n",
        "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
        "\n",
        "def train_epoch(model, lr, params):\n",
        "  \"\"\"Update the parameters using the gradient of loss with respect to the parameter.\"\"\"\n",
        "  for x, y in dl:\n",
        "    calc_grad(x, y, model)\n",
        "    for p in params:\n",
        "      p.data -= p.grad * lr\n",
        "      p.grad.zero_() # reset gradient, as PyTorch accumulates by default\n",
        "\n",
        "def batch_accuracy(x, y):\n",
        "  preds = x.softmax(dim=1)\n",
        "  predicted_classes = preds.argmax(dim=1)\n",
        "  correct = predicted_classes == y\n",
        "  return correct.float().mean()\n",
        "\n",
        "def validate_epoch(model):\n",
        "  accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n",
        "  return round(torch.stack(accs).mean().item(), 4)\n",
        "\n",
        "for i in range(20):\n",
        "  train_epoch(matrix_x, 0.1, params)\n",
        "  print(validate_epoch(matrix_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnC4oJxyjTCi",
        "outputId": "cea86847-322b-4170-928c-b9ffae87f7c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1763\n",
            "0.2779\n",
            "0.3708\n",
            "0.436\n",
            "0.4744\n",
            "0.5022\n",
            "0.5237\n",
            "0.5437\n",
            "0.5571\n",
            "0.5701\n",
            "0.5803\n",
            "0.5882\n",
            "0.5981\n",
            "0.605\n",
            "0.6114\n",
            "0.6168\n",
            "0.6223\n",
            "0.628\n",
            "0.6332\n",
            "0.6374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is now a working linear learner model working at about 63% accuracy. It is not yet a neural network as we have not introduced non-linearity - this is expected to drastically improve its predictive power."
      ],
      "metadata": {
        "id": "GyyB16Uojirb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Function\n",
        "\n",
        "For the model to improve, we need to add more **layers** and non-linearity through **activation functions**. Linear classifiers are constrained in terms of their predictive power - to make it more complex to handle more tasks we need to make it a **neural network** (NN). A NN is a computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons) that process and transform input data through weighted connections, enabling it to learn complex patterns and make predictions or classifications.\n",
        "\n",
        "We will do this by making our model 3 layers.\n",
        "1. Layer 1 will be a **linear layer**.\n",
        "2. Layer 2 will be an **activation layer**.\n",
        "3. Layer 3 will be a **linear layer**.\n",
        "\n",
        "A **linear layer** also known as a **fully connected layer** linearly transforms the input by applying the parameters. This is what our linear model above does through the equation `y = WX + b` where:\n",
        "- `y` is the transformed data.\n",
        "- `W` is the weights matrix.\n",
        "- `b` is the bias vector.\n",
        "- `X` is the input features.\n",
        "\n",
        "An **activation layer** introduces non-linearity in the model, enabling it to learn and model complex patterns. For example, the **Rectified Lienar Unit** (ReLU) function is a common activation function (makes negative values 0)."
      ],
      "metadata": {
        "id": "uo48EHq8KWkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_net = nn.Sequential (\n",
        "    nn.Linear(28*28, 30), # Layer 1\n",
        "    nn.ReLU(), # Layer 2\n",
        "    nn.Linear(30, 10) # Layer 3\n",
        ")"
      ],
      "metadata": {
        "id": "IXRp7uPJmQ3V"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first layer, we have 28*28 weights for each pixel in the MNIST images. We used 30 as a somewhat arbtirary hyperparameter (the number of neurons) as the output size. We would typically refine this parameter through experimentation. This value allows the model to learn more complex patterns.\n",
        "\n",
        "In the second layer, we use the ReLU activation function to add nonlinearity.\n",
        "\n",
        "In the third layer, we have 30 input, representing the 30 **activations** from  layer 1. Activations are the numbers that are calculated and returned by each linear or activation (non-linear) layer. The output size of 10 corresponds to the number of classes in the MNIST dataset (0 to 9). This layer will output a vector of 10 values, representing the network's \"confidence\" (**logits**) for each class.\n",
        "\n",
        "We will be using the fastai model to make the NN. As such, we need to define a `DataLoaders` object with our training and validation sets."
      ],
      "metadata": {
        "id": "Xcz6SVbLnX8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls = DataLoaders(dl, valid_dl)"
      ],
      "metadata": {
        "id": "kQyqGVKknnDJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, simple_net, opt_func=SGD, loss_func=calc_loss, metrics=batch_accuracy)\n",
        "learn.fit(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "XDzFWN33mD5O",
        "outputId": "8e88034b-a3b8-4e29-f496-b96502a9402a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>batch_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.741871</td>\n",
              "      <td>1.649581</td>\n",
              "      <td>0.609734</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.681991</td>\n",
              "      <td>1.576644</td>\n",
              "      <td>0.623380</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.621332</td>\n",
              "      <td>1.505009</td>\n",
              "      <td>0.636084</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.561149</td>\n",
              "      <td>1.435615</td>\n",
              "      <td>0.652066</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.501911</td>\n",
              "      <td>1.369272</td>\n",
              "      <td>0.664831</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.444192</td>\n",
              "      <td>1.306521</td>\n",
              "      <td>0.676434</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.388400</td>\n",
              "      <td>1.247706</td>\n",
              "      <td>0.687202</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.334945</td>\n",
              "      <td>1.192948</td>\n",
              "      <td>0.694479</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.284119</td>\n",
              "      <td>1.142238</td>\n",
              "      <td>0.701471</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.236120</td>\n",
              "      <td>1.095436</td>\n",
              "      <td>0.707382</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.191044</td>\n",
              "      <td>1.052333</td>\n",
              "      <td>0.711367</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.148897</td>\n",
              "      <td>1.012672</td>\n",
              "      <td>0.716130</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.109584</td>\n",
              "      <td>0.976198</td>\n",
              "      <td>0.720223</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.073011</td>\n",
              "      <td>0.942634</td>\n",
              "      <td>0.724736</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.039083</td>\n",
              "      <td>0.911713</td>\n",
              "      <td>0.728536</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.007606</td>\n",
              "      <td>0.883194</td>\n",
              "      <td>0.731534</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.978372</td>\n",
              "      <td>0.856872</td>\n",
              "      <td>0.735642</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.951208</td>\n",
              "      <td>0.832540</td>\n",
              "      <td>0.739304</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.925926</td>\n",
              "      <td>0.810006</td>\n",
              "      <td>0.742290</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.902398</td>\n",
              "      <td>0.789095</td>\n",
              "      <td>0.744988</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using a NN instead of a linear learner model, we have been able to improve the accuracy to about 75%. If we wanted to further improve our model, we could utilise pre-trained models such as resnet18. However, this process has demonstrated the predictive power of the NN.\n",
        "\n",
        "While the model achieved reasonable accuracy, there is a risk of **overfitting**, especially as the model becomes more complex.Overfitting occurs when a model learns to perform exceptionally well on the training data but fails to generalize to new, unseen data, often capturing noise or irrelevant patterns instead of the underlying trends."
      ],
      "metadata": {
        "id": "jRul7KQbn4Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "GyJv_wfXoQrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is why deep learning seems magical:\n",
        "1. The neural network can solve any problem to any level of accuracy given the correct set of parameters.\n",
        "2. There is a way to find the best set of parameters for any function called stochastic gradient descent."
      ],
      "metadata": {
        "id": "vlGUr9DAqRG0"
      }
    }
  ]
}